---
layout:     post
title:      PID Control
date:       2018-04-14 0:00:00
summary:    PID control is the most ubiquitous form of control out there. Here we relate it to familiar concepts in optimization.
author:     Ben Recht
visible:    false
blurb: 		  true
---

Many popular methods in controls are actually very model free. Indeed, the most ubiquitous control scheme, PID control, has only three parameters. PID stands for "proportional integral derivative" control. The idea behind PID control is pretty simple: suppose you have some dynamical system with a single input that produces a single output. Let's say you'd like the output to read some constant value $y_t = v$. For instance, you'd like to keep the water temperature in your espresso machine at precisely [203 degrees Fahrenheit](http://espressovivace.com/education/espresso-tips/), but you don't have a precise differential equation modeling your entire kitchen. PID control works by creating a control signal based on the error $e_t=v-y_t$.  As the name implies, the control signal is a combination of error, its derivative, and its integral:

$$
	u_t = k_P e_t + k_I \int_0^t e_s ds + k_D \dot{e}_t\,.
$$

I've heard differing accounts, but somewhere in the neighborhood of 90 and 96 percent of all control systems are PID. And some suggest that the number of people using the "D" term is negligible. Something like 95 percent of the myriad collection of control processes that keep our modern society running are configured by setting _two_ parameters.

In some sense, PID control is the "gradient descent" of control: it solves most problems and fancier methods are only needed for special cases. The odd thing about statistics and ML research these days is that everyone knows about gradient descent, but almost none of the ML researchers I've spoken to knows anything about PID control. So perhaps to explain the ubiquity of PID control to the ML crowd, it might be useful to establish some connections to gradient descent.

## PID in discrete time

Before we proceed, let's first make the PID controller digital. We all design their controllers in discrete time rather than continuous time since we do things on computers. How can we discretize the PID controller?  First, we can compute the integral term with a running sum:

$$
	w_{t+1} = w_t + e_t
$$

When $w_0=0$, then $w_t$ is the sum of the sequence $e_s$ for $s<t$.

The derivative term can be approximated by finite differences. But since taking derivatives can amplify noise, most practical PID controllers actually filter the derivative term to damp this noise. A simple way to filter the noise is to let the derivative term be a running average:

$$
	v_{t} = \beta v_{t-1} + (1-\beta)(e_t-e_{t-1})\,.
$$

Putting everything together, a PID controller in discrete time will take the form

$$
	u_t = k_P e_t + k_I w_t + k_D v_t
$$

# Integral Control

Let's now look at pure integral control. We can simplify the controller in this case to have the form

$$
	u_t = u_{t-1}+k_i e_t\,.
$$

This should look very familiar to all of my ML friends out there as it looks an _awful lot_ like gradient descent. To make the connection crisp, suppose $y= f'(u)$ for some fixed function $f$. And let's suppose we want to drive $y_t$ to zero so that $e = -f'(u)$. Then when hooked in feedback with this plant, integral control _is_ gradient descent. Just like in gradient descent, integral control can never give you the wrong answer. If you converge to a constant value of the control parameter, then the error must be zero.

## Proportional Integral Control

As discussed above, PI control is the most ubiquitous form of control. For optimization, it is less common, but still finds a valid algorithm when $e = -f'(u)$.

Doing a variable substitution, the $PI$ controller will take the form

$$
	u_{t+1} = u_t + (k_I-k_P) e_t + k_P e_{t+1}
$$

If $e_t = -f'(u_t)$, then we get the algorithm:

$$
	u_{t+1} + k_P f'(u_{t+1}) = u_t - (k_I-k_P) f'(u_t)
$$

This looks a bit tricky as somehow we need to compute the gradient of $f$ at our current time step. However, optimization friends out there will note that this equation is the optimality conditions for the algorithm

$$
	u_{t+1} = \mathrm{prox}_{k_P f} ( u_t - (k_I-k_P) f'(u_t) )\,.
$$

Hence, PI control combines a gradient step with a proximal step. The algorithm is a hybrid between the classical proximal point method and gradient descent. Note that if this method converges, it will again converge to a point where $f'(u)=0$.

## Proportional Integral Derivative Control

The master algorithm is PID. What happens here? We can do a clever change of variables defining the auxiliary variable

$$
	x_t = \frac{1}{1-\beta}w_t+\frac{\beta}{(1-\beta)^3}v_t-\frac{\beta}{(1-\beta)^2}e_t\,.
$$

With this definition, the PID controller reduces to the tidy set of equations:

$$
\begin{aligned}
	x_{t+1} &= (1+\beta)x_t -\beta x_{t-1} + e_t\\
	u_t &=  C_1 x_t + C_2 x_{t-1}+  C_3 e_t\,.
\end{aligned}
$$

The coefficients $C_i$ are given by the formulae:

$$
\begin{algined}
C_1 &= -(1-\beta)^2 k_D+k_I\\
C_2 &= (1-\beta)^2 k_D-\beta k_I\\
C_3 &= (1-\beta)^2 k_D-\beta k_I
\end{aligned}
$$

The $x_{t}$ sequence looks like a _momentum_ sequence used in optimization. Indeed, with proper settings of the gains, we can recover a variety of algorithms that we commonly use in machine learning. \emph{Gradient descent with momentum} with learning rate $\alpha$----also known as the \emph{Heavy Ball method}---is realized with the settings.

$$
	k_I = \frac{\alpha}{1-\beta} ~~~~ k_D=\frac{\alpha \beta}{(1-\beta)^3}~~~~ k_P = \frac{-\alpha \beta}{(1-\beta)^2}
$$

Nesterov's accelerated method pops out when we set

$$
	k_I = \frac{\alpha}{1-\beta} ~~~~ k_D=\frac{\alpha \beta^2}{(1-\beta)^3}~~~~ k_P = \frac{-\alpha \beta^2}{(1-\beta)^2}
$$

These are remarkably similar, differing only in the power of $\beta$ in the numerator of the proportional and derivative terms.

## The Lur'e Problem

It's quite remarkable that most of the popular algorithms in ML end up being special cases of PID control. It seems weird to be analyzing feedback with some non-dynamic nonlinear map, it turns out that the problem has a long history in controls. It is a fundamental object of study in control theory, called the Lur'e problem. Finding a controller to push a static nonlinear system to a fixed point turns out to be identical to designing an optimization algorithm to set a gradient to zero. Laurent Lessard,  Andy Packard, and I made these connections in [our paper](xxx), showing that many of the rates of convergence for optimization algorithms could be derived using stability techniques from controls. We also used this approach to show that the Heavy Ball method might not always converge at an accelerated rate, justifying why we need the slightly more complicated Nesterov accelerated method for reliable performance. With Robert Nishihara and Mike Jordan, we followed up this work showing that you could even use this to study ADMM using the connections between prox-methods and proportional integral control. Bin Hu, Pete Seiler, and Anders Rantzer [generalized this technique to understand better understand stochastic optimization methods](xxx). And Laurent and Bin [made the formal connections to PID control](xxx) that I discuss in this post.

## Learning to learn

With the connection to PID control in mind, we can think of learning rate tuning as controller tuning. The Nichols-Ziegler rules (developed in the forties) simply find the largest gain $k_P$ such that the system oscillates, and set the PID parameters based on this gain and the frequency of the oscillations. A common trick for gradient descent tuning is to find the largest value such that gradient descent does not diverge, and then set the momentum and learning rate accordingly from this starting point.

Similarly,  we can think of the "learning to learn" paradigm in machine learning as a special case of controller design. Though PID works for most applications, it's possible that a more complex controller will work for a particular application. In the same vein, it's always possible that there's something better than Nesterov's method if you restrict your set of instances. And maybe you can even find this controller by gradient descent. But it's always good to remember, 95\% is still PID.

Though PID is a powerful workhorse, it is typically used for simple low-level control loops attempting to maintain some static equilibrium. It seems like it's not particularly useful for more complex tasks like robotic acrobatics. However, in the next post, I will describe a more complex control task that can also be solved by PID-type techniques.
