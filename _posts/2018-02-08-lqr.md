---
layout:     post
title:      The Linear Quadratic Regulator
date:       2018-01-29 7:00:00
summary:    An outsider tour of reinforcement learning, Part 2. The Linear Quadratic Regulator.
author:     Ben Recht
visible:    false
---

NOISE


What would be a dead simple baseline for understanding optimal control with unknown dynamics and providing perspectives on reinforcement learning?

Let's start with too much generality. The generic optimal control problem takes the form:

$$
\begin{array}{ll}
\mbox{maximize} & \mathbb{E}_{e}[ \sum_{t=0}^N R_t[x_t,u_t] ]\\
\mbox{subject to} &	x_{t+1} = f(x_t, u_t, e_t)
\end{array}
$$

The first step towards simplicity would be to study a case when this problem is convex. Though there are exceptions, the only constraints that are generally guarantee convexity are the linear ones. For this problem to be linear, the dynamical system must be of the form

$$
x_{t+1} = A_t x_t +  B_t u_t + e_t
$$

Such dynamical systems play a central role in control theory and are called _linear dynamical systems_ (not the most creatively named class, but we'll give that a pass).

Though linear constraints are somewhat restrictive, many systems are linear near over the range at which we'd like them to perform. Indeed, lots of engineering effort goes into engineering systems so that their responses are as close to linear as possible.

Note that the quadrotor dynamics we [derived from Newton's Laws](xxx) are linear. The state of the quadrotor is its vertical position and velocity $x_t = [z_t;v_t]$. The input $u_t$ is the propeller force. Newton's Laws written in matrix form are thus

$$
	x_{t+1} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} x_t
	+ \begin{bmatrix} 0 \\ 1/m \end{bmatrix} (u_t-g)
$$

What about cost functions? The thing about cost functions, and this is a recurring theme in reinforcement learning, is that they are completely made up. There are a variety of costs that we can define in order to achieve a control objective. If we want the quadrotor to fly to location $z_f$, we could specify

$$
	R_t[x_t,u_t] = |z_t-z_f|
$$

or

$$
	R_t[x_t,u_t] = (z_t-z_f)^2
$$

or

$$
	R_t[x_t,u_t] = \begin{cases} 1 & z_t=z_f\\
	0 & \mbox{otherwise}
	\end{cases}
$$

Which of these is the best? There is a tradeoff here between the character of the controller output and the ease of computing this controller. Since we're designing our costs, there's not reason not to restrict our attention to costs that would be easy to solve.

With this degree of freedom in mind, let me again put on my optimizer hat and declare that convex quadratic costs are always the first set of instances I'd look at when evaluating an optimization algorithm. Hence, we may as well assume that $R[x_t,u_t]$ is a convex, quadratic function.

With this in mind, let me propose that the simplest baseline to begin studying optimal control and RL is the *Linear Quadratic Regulator*:

$$
\begin{array}{ll}
\mbox{minimize}_{u_t,x_t} \, & \frac{1}{2}\sum_{t=0}^N \left\{x_t^TQ x_t + u_t^T R u_t\right\}  \\
& \qquad + \frac{1}{2} x_{N+1}^T S x_{N+1}, \\
\mbox{subject to} & x_{t+1} = A x_t+ B u_t, \\
& \qquad \mbox{for}~t=0,1,\dotsc,N,\\
& \mbox{($x_0$ given).}
\end{array}
$$

As we've discussed, we can pose flying a quadrotor as an instance of LQR. We can first center our variables, letting $u_t \leftarrow u_t-g$ and $z_t \rightarrow z_t-z_f$. Then trying to reach point $0$ can be written with

$$
	Q = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \qquad\qquad R = r_0
$$

for some scalar $r_0$.  The $R$ term here penalizes too much propeller force (because this might deplete your batteries). Note that even in LQR there is an element of design: changing the value of $r_0$ will change the character of the control to balance battery life versus speed or reaching the desired destination.

## Adjoints in Optimal Control

My favorite way to derive the optimal control law in LQR uses the methods of adjoints, known by the cool kids these days at backpropagation.  I actually worked through the LQR example in my [post on backprop](http://www.argmin.net/2016/05/18/mates-of-costate/) from a couple of years ago.  And, as I mentioned there, the method of adjoints has its roots deep in optimal control.

The first step to computing the control policy is to form a Lagrangian. We'll then try to find a setting of the variables $u$ $x$ and the Lagrange multipliers $p$ such that the derivatives vanish with respect to all of the variables.  The Lagrangian for the LQR problem has the form

$$
\begin{aligned}
\mathcal{L} (x,u,p) &:= \sum_{t=0}^N \left[ \frac{1}{2} x_t^T Q x_t +  \frac{1}{2}u_t^T R u_t \right.\\
&\qquad\qquad \left. - p_{t+1}^T (x_{t+1}-A x_t - B u_t) \right]\\
&\qquad\qquad +\frac{1}{2} x_{N+1}^T S x_{N+1}.
\end{aligned}
$$

The gradients of the Lagrangian are given by the expressions

$$
\begin{aligned}
\nabla_{x_t} \mathcal{L} &= Qx_t - p_{t} + A^T p_{t+1} , \\
\nabla_{x_{N+1}} \mathcal{L} &= -p_{N+1} +  S x_{N+1} , \\
\nabla_{u_t} \mathcal{L} &= R u_t + B^T p_{t+1} , \\
\nabla_{p_t} \mathcal{L} &= -x_{t} + Ax_{t-1} + B u_{t-1}.
\end{aligned}
$$

We now need to find settings for all of the variables to make these gradients vanish. Note that to satisfy $\nabla_{p_i} \mathcal{L}=0$, we simply need our states and inputs to satisfy the dynamical system model. The simplest way to solve for the costates $p$ is to work backwards. Note that the end co-state satisfies

$$
	p_{N+1} = S x_{N+1}
$$

the final co-state is a linear function of $x_{N+1}$. We can show by reverse induction that

$$
	p_{t} = M_t x_{t}
$$

for some matrix $M_t$. That is, the costate is a linear function of $x_t$. To see this note that by the inductive hypothesis

$$
	p_{t+1} = M_{t+1} x_{t+1} = M_{t+1} (Ax_t + B u_t )
	= M_{t+1} Ax_t - M_{t+1} BR^{-1} B^T p_{t+1} )
$$

So that

$$
	p_{t+1} = (I+M_{t+1}BR^{-1} B^T )^{-1} M_{t+1} Ax_t
$$

Now, $\nabla_{x_t} \mathcal{L}=0$ implies

$$
	p_{t} = \left(Q + A^T p_{t+1} = Q + A^T (I+M_{t+1}BR^{-1} B^T )^{-1} M_{t+1} A\right) x_t
$$

proving the claim.

Note that this algebraic equation fruther reveals that $u_t$ itself is a linear function of $x_t$.  Indeed, we have

$$
	u_t = -R^{-1} B^Tp_{t+1} = -R^{-1}B^T (I+M_{t+1}BR^{-1} B^T )^{-1} M_{t+1} A x_t
$$

Hence, to compute the control $u_t$, we can run a reverse recursion to define $M$:
$M_{N+1} = S$ and

$$
M_t =  Q + A^T (I+M_{t+1}BR^{-1} B^T )^{-1} M_{t+1} A
$$

and then we can run a forward pass to compute $u_t$ using these computed values of $M_t$.

Note that if we run the recursion for $M_t$ on a longer and longer time horizon, letting $N$ tend to infinity, we get a fixed point equation for $M$:

$$
M =  Q + A^T (I+M BR^{-1} B^T )^{-1} M A
$$

This equation defines $M$ and is called the _Discrete Algebraic Riccati Equation_. Note that on an infinite time horizon, we also have that the optimal control policy is to set

$$
	u_t = -K x_t
$$

where $K$ is a fixed matrix defined by

$$
	R^{-1}B^T (I+M BR^{-1} B^T )^{-1} M A
$$

That is, for LQR on an infinite time horizon, the optimal control policy is _static_, and can be computed once online and run for infinite time.  However, on a finite time horizon where $N$ is not too large, a static policy may yield a very suboptimal control.


## LQR for the position problem.

How does the LQR controller look for

XXX Quadrotor


## Takeaways

Again, I emphasize that LQR cannot capture every interesting optimal control problem, not even when the dynamics are linear. For example, the rotors canâ€™t apply infinite forces, so some sort of constraints must be enforced on the control actions.
















There have been four thousand new frameworks for deep learning thrown on the market the past year, and I bet you were wondering what you needed to jump into this hot marketplace.  Essentially, there are two components required for most mortals who aim to train neural nets: a unit that efficiently computes derivatives of functions that are compositions of many sub-functions and a unit that runs stochastic gradient descent.  I can write the stochastic gradient descent part in ten lines of python.  I'll sell it to the highest bidder in the comments.  But what about the automatic differentiator?

Automatic differentiation does seem like a bit of a black box.  Some people will just scoff and say "it's just the chain rule." But evaluating the chain rule efficiently requires being careful about reusing information, and not having to handle special cases.  The backpropagation algorithm handles these recursions well.  It is a dynamic programming method to compute derivatives, and uses clever recursions to aggregate the gradients of the components.   However, I always find the derivations of backprop to be confusing and too closely tied to neuroscientific intuition that I sorely lack.  Moreover, for some reason, dynamic programming always hurts my brain and I have to think about it for an hour before I remember how to rederive it.  

A few years ago, [Steve Wright](http://pages.cs.wisc.edu/~swright/) introduced me to an older method from optimal control, called the method of adjoints, which is equivalent to backpropagation.  It's also easier (at least for me) to derive. This is because the core of the method is *Lagrangian duality*, a topic at the foundation of everything we optimizers do.

## Deep neural networks

Before we get to Lagrangian duality, we need a constrained optimization problem.  There's no Lagrangian without some constraints!  So let's transform a deep learning optimization problem into a constrained optimization problem.

The standard deep learning goal is to solve optimization problems of the form

$$
	\begin{array}{ll}
		\mbox{minimize}_{\varphi} &\frac{1}{n} \sum_{k=1}^n \mathrm{loss}(\varphi(x_k),y_k) ,
	\end{array}
$$

where $\varphi$ is a function from features to labels that has an appropriate level of expressivity.  In deep learning, we assume that $\varphi$ is a giant composition:

$$
	\varphi(x;\vartheta) = f_\ell \circ f_{\ell-1} \circ f_{\ell-2} \circ \cdots  \circ f_1(x)
$$

and each $f_i$ has a vector of parameters $\vartheta_{i-1}$ which may be optimized.  In this case, we can rewrite the unconstrained minimization problem as a constrained one:

$$
	\begin{array}{ll}
		\mbox{minimize}_{\vartheta} &\frac{1}{n} \sum_{k=1}^n \mathrm{loss}(z_k^{(\ell)},y_k) \\
		\mbox{subject to} & z_k^{(\ell)} = f_\ell(z_{k}^{(\ell-1)}, \vartheta_{\ell})\\
		&  z_k^{(\ell-1)} = f_{\ell-1}(z_{k}^{(\ell-2)}, \vartheta_{\ell-1})\\
		& \vdots\\
		&  z_k^{(1)} = f_1(x_k, \vartheta_{1}).
	\end{array}
$$

Why does this help?  Explicitly writing out the composition in stages is akin to laying out a computation graph for the function.  And once we have a computation graph, we can use it to compute derivatives.

## The method of adjoints

The method of adjoints reveals the structure of the backpropagation algorithm by constructing a Lagrangian and computing the KKT conditions for the constrained optimization formulation.  To simplify matters, let's restrict our attention to the case where $n=1$. This corresponds to when there is a single $(x,y)$ data pair as you'd have if you were running stochastic gradient descent.

To derive the KKT conditions we first form a Lagrangian function with Lagrange multipliers $p_i$:

$$
\begin{aligned}
\mathcal{L} (x,u,p) &:=   \mathrm{loss}(z^{(\ell)},y) \\
&\qquad\quad  - \sum_{i=1}^{\ell} p_i^T(z^{(i)} - f_i(z^{(i-1)},\vartheta_i)),
\end{aligned}
$$

The derivatives of this Lagrangian are given by the expressions:

$$
\begin{aligned}
\nabla_{z^{(i)}} \mathcal{L} &= - p_{i} + \nabla_{z^{(i)}} f_{i+1}(z^{(i)},\vartheta_{i+1})^T p_{i+1} , \\
\nabla_{z^{(\ell)}} \mathcal{L} &= -p_\ell + \nabla_{z^{(\ell)}} \mathrm{loss}(z^{(\ell)},y) , \\
\nabla_{\vartheta_i} \mathcal{L} &= \nabla_{\vartheta_i} f_i(z^{(i-1)},\vartheta_i)^Tp_i ,\\
\nabla_{p_i} \mathcal{L} &= z^{(i)} - f_i(z^{(i-1)},\vartheta_i).
\end{aligned}
$$

The Lagrange multipliers $p_i$ are also known as the *adjoint variables* or *costates*. To compute the gradient, we just have to solve the set of nonlinear equations

$$
\nabla_{p_i} \mathcal{L} = 0~\mbox{and}~ \nabla_{z_i} \mathcal{L} =0
$$

and then we can just read off the gradient with respect to $\nabla_\vartheta \mathrm{loss}(\varphi(x;\vartheta),y)= \nabla_{\vartheta_i} f_i(z^{(i-1)},\vartheta_i)^Tp_i$.
(I'll explain why later... trust me for a second).

The structure here is particularly nice.  If we solve for $\nabla_{p_i} \mathcal{L}=0$, this just amounts to satisfying the constraints $z^{(i)} = f_i(z^{(i-1)})$.  This is called the *forward pass*.  We can then compute $p_i$ from the equations $\nabla_{z_i} \mathcal{L} =0$.  That is,

$$
p_\ell = \nabla_{z^{(\ell)}} \mathrm{loss}(z^{(\ell)},y) \,.
$$

This is the *backward pass*.  The gradients with respect to the parameters can then be computed by adding up linear functions of the adjoint variables.

What is particularly nice about the method of adjoints is that it suggests a convenient set of working variables that enable fast gradient computation.  It explicitly builds in a caching strategy for subunits of the computation.  Two different constrained formulations will lead to different computation graphs and sets of costates, but they will return the same gradient.

There are tons of ways to generalize this.  We could have a more complicated computation graph.  We could share variables among layers (this would mean adding up variables).  We could penalize hidden variables or states explicitly in the cost function.  Regardless, we can always read off the solution from the same forward-backward procedure.   The computation graph always provides a  "forward model" describing the evolution of an input to the output. The adjoint equation involves the adjoint ("transpose") of the Jacobians of this equation, which measures the sensitivity of one node to the previous node.  
