---
layout:     post
title:      Large Margin Micro Blogging
date:       2016-04-26 7:00:00
summary:    How much VC dimension can you cram in a tweet?
author:     Ben Recht
visible:    true
---

While intricate deep models are all the rage in machine learning, [my last post](http://www.argmin.net/2016/04/18/bottoming-out/) tried to make the case that we still need to care about model regularization. As it has been since the dawn of time, choosing simple models is still our best weapon against overfitting.   But what does “simple” mean exactly?  Of the many ways to measure simplicity, I’ve always been partial to the “minimum description length” principle: the fewer lines of code needed to train, store, and evaluate your model, the more robust that model likely is.


