---
layout:     post
title:      Make It Happen
date:       2018-01-29 0:07:00
summary:    An outsider tour of reinforcement learning, Part 1. RL as prescriptive analytics.
author:     Benjamin Recht
blurb: true
visible:    true
---

*This is the first part of ["An Outsider's Tour of Reinforcement Learning."](http://www.argmin.net/outsider-rl.html)  Part 2 is [here](http://www.argmin.net/2018/02/01/control-tour/).*

If you read hacker news, you’d think that deep reinforcement learning can be used to solve any problem.  Deep RL has claimed to [achieve superhuman performance on Go](https://deepmind.com/research/alphago/), [beat atari games](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/), [control complex robotic systems](https://blog.openai.com/generalizing-from-simulation/), [automatically tune deep learning systems](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html), [manage queueing in network stacks](https://www.microsoft.com/en-us/research/publication/resource-management-deep-reinforcement-learning/), and [improve energy efficiency in data centers](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/). What a miraculous technology! I personally get suspicious when audacious claims like this are thrown about in press releases, and I get even more suspicious when [other researchers call into question their reproducibility](https://arxiv.org/abs/1709.06560). I want to take a few posts to unpack what is legitimately interesting and promising in RL and what is probably just hype. I also want to take this opportunity to argue in favor of more of us working on RL: some of the most important and pressing problems in machine learning are going to require solving exactly the problems RL sets out to solve.

Reinforcement learning trips people up because it requires thinking about core concepts that are not commonly discussed in a first course on machine learning. First, you have to think about statistical models that evolve over time, and understand the nature of dependencies in data that is temporally correlated. Second, you need to understand feedback in statistical learning problems, and this makes all of the analysis challenging.  When there is feedback, the distribution of the observations changes with every action taken, and an RL system must adapt to these actions.  Both of these are complex challenges, and I’m going to spend time discussing both. Indeed, I can use both of these concepts to introduce RL from seemingly unrelated starting points. In this post, I’ll dive into RL as a form of _predictive analytics_. In the next post, I’ll describe RL as a form of _optimal control_. Both derivations will highlight how RL is different from the machine learning with which we are most familiar.

## Reinforcement Learning As Predictive Analytics

Chris Wiggins introduced a [brilliant taxonomy of ML](https://www.slideshare.net/chrishwiggins/machine-learning-summer-school-2016/75) that I find rather clarifying.

There are three main pillars of machine learning: unsupervised, supervised, and reinforcement learning. There are few other kinds of machine learning that are connected to one of these three core categories---notably semi-supervised learning and active learning---but I think this trichotomy nicely covers most of the current research in machine learning.

{: .center}
![The ML Taxonomy](/assets/rl/taxonomy.png){:width="420px"}

What is the difference between each of these?  In all three cases, you are given access to some table of data where the rows index examples and the columns index features (or attributes) of the data.  

In unsupervised learning, the goal is to summarize the examples. We can say that each row has a list of attributes **_x_**, and the goal is to create a shorter list of attributes **_z_** for each example that somehow summarizes the salient information in **_x_**. The features in **_z_** could be assignment to clusters or some sort of mapping of the example into a two dimensional state for plotting.

{: .center}
![recent bedroom](/assets/rl/bedroom.png){:width="160px" height="160px"}
![hot topic](/assets/rl/ml-word-cloud.jpg){:width="160px" height="160px"}
![so delicious](/assets/rl/swiss-roll.png){:width="160px" height="160px"}

In supervised learning, one of the columns is special. This is the feature which we’d like to predict from the other features. The goal is to predict **_y_** from **_x_** such that on new data you are accurately predicting **_y_**. This is the form of machine learning we’re most familiar with and includes classification and regression as special cases.

In reinforcement learning, there are two special columns, **_a_** and **_r_**. The goal is to analyze **_x_** and then subsequently choose **_a_** so that **_r_** is large. There are an endless number of problems where this formulation is applied from online decision making in games to immoral engagement maximization on the web.

The broader field of data science has terminology for all of these analytical procedures as well. Indeed, much of ML fits cleanly inside bins of data analytics, laid out in terms of difficulty and value in this chart which I have adapted from a famous infographic of Gartner:

{: .center}
![infographix, yo.](/assets/rl/gartner.png){:width="450px"}

_Descriptive_ analytics refers to summarizing data in a way to make it more interpretable. Unsupervised learning is a form of descriptive analytics. _Predictive_ analytics aims to estimate outcomes from current data. Supervised learning is a kind of predictive analytics. Finally, _prescriptive_ analytics guides actions to take in order to guarantee outcomes. RL as described here falls into this bucket.

Note that the value assessments in this chart seems to fly in the face of conventional wisdom in machine learning (e.g., [1](https://www.axios.com/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524-f619efbd-9db0-4947-a9b2-7a4c310a28fe.html), [2](https://twitter.com/ylecun/status/701189938965041152), [3](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckep3z6/),
[4](https://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-set-artificial-intelligence-free/) ). But the conventional wisdom is wrong, and it’s important that we correct it. According to Gartner, business leaders, and your humble blogger, _unsupervised_ learning is by far the easiest of the three types of machine learning problems because the stakes are so low. If all you need to do is summarize, there is no wrong answer. Whether or not your bedrooms are rendered correctly by a GAN has no impact on anything. Descriptive analytics and unsupervised learning lean more on aesthetics and less on concrete targets. Predictive analysis and supervised learning are _more_ challenging as we can evaluate accuracy in a principled manner on new data.

The most challenging form of analytics and the one that can return the most value is prescriptive analytics. The value proposition is clear: prescriptive analysis and reinforcement learning demand interventions with the promise that these actions will directly lead to valuable returns. Prescriptive analysis consumes new data about and uncertain and evolving environment, makes predictions, and uses these predictions to impact the world. Such systems promise plentiful rewards for good decisions, but the complicated feedback arising from the interconnection is hard to study in theory, and failures can lead to catastrophic consequences. In real computing systems, whether they be  autonomous transportation system or seemingly mundane social engagement systems like Facebook, actively interacting with reality has considerably higher stakes than scatter plotting a static data set for a PowerPoint presentation.

Indeed, this is why I’ve been so obsessed with understanding RL for the past few years. RL provides a useful framework to conceptualize interaction in machine learning. As I’ve been hammering on for a while now, we have to take responsibility for our machine learning systems and understand what happens when we set them loose on the world. The stakes couldn’t be higher, and I think that understanding a bit more about RL can help us build safer machine learning systems in general. For these reasons, I do hope you’ll humor me in sticking it out for my outsider’s tour of the area. You can start with [Part 2](http://www.argmin.net/2018/02/01/control-tour/), where I describe reinforcement learning from the perspective of optimal control.
