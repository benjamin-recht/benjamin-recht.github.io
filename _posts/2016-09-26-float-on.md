---
layout:     post
title:      Float On
date:       2016-06-10 7:00:00
summary:    the lack of precision of float32 matrix analysis
author:     Ben Recht
visible:    false
---

Oh man, a three month gap between posts!  That's just sad.  Life comes at you fast as the kids are all saying.  Well, I'm back for the two of you out there who missed me.

float32 is attractive because you get "speed-ups" essentially for free.  On CPUs this is almost entirely due to SIMD effects: you can fit twice as many float32 numbers in an instruction as doubles, and counting SIMD instructions is typically a better estimate for running time than counting flops.  On GPUs float32 gets an additional benefit.  Because of the layout of GPU hardware, xxx.

Since running large physics simulations or training large neural networks is such a time intensive task, eating a 3x slowdown for extra precision is simply not an option unless the numerical stability becomes an issue.  Cursory reading of numerical analysis textbooks suggest that these stability issues only happen if you need to invert a matrix.  For *forward models* like ODE and gradient descent solvers, it seems like there shouldn't be anything wrong with float32 in principle.

The last paragraph captures what I certainly believed to be true until recently.  But I've been playing with some simple models that come up in training neural networks, and I'm less convinced than I used to be.

Let's start with a deceptively simple example: computing the column sum of a $d \times n$ matrix $A$.

Suppose we have partitioned the columns of $A$ so that $A=[A1,A2]$ and $A1$ and $A2$ have equal size.   Let $e_1$ be the $n\times 1$ vector of all ones and $e_2$ be the $n/2\times 1$ vector of all ones.  Here are three possibilities for computing the column sum:

v1 = np.sum(A,axis=1)

v2 = np.dot(A,e1)

v3 = np.dot(A1,e2) + np.dot(A2,e2)

v4 = np.dot(A1+A2,e2)

These should, if life was grand, all return the same answer.  But sadly, they do not.  If we choose $A$ at random, setting $d=2001$ and $n=2000$, we get the following errors for float32:

trial	   err 1 |  err 2  |  err 3  |  err 4  |
------------------------------------------------
    0	 4.8e-05 | 2.8e-04 | 9.5e-05 | 7.4e-05 |
    1	 1.8e-04 | 5.3e-04 | 6.1e-04 | 3.2e-04 |
    2	 3.2e-04 | 8.0e-04 | 1.6e-04 | 1.8e-04 |
    3	 3.0e-03 | 3.0e-03 | 2.9e-04 | 2.3e-03 |
    4	 1.0e-03 | 1.2e-03 | 3.9e-04 | 4.9e-04 |
    5	 1.3e-03 | 2.1e-03 | 3.3e-02 | 1.2e-02 |
    6	 1.7e-04 | 6.2e-04 | 4.9e-04 | 4.5e-04 |
    7	 4.7e-04 | 5.6e-04 | 2.8e-04 | 1.2e-03 |
    8	 1.3e-03 | 8.4e-03 | 5.7e-03 | 6.6e-03 |
    9	 2.1e-03 | 2.4e-03 | 1.1e-03 | 1.5e-03 |

Explain relative error

Here the errors are computing against the baseline sum computed in float64.  These are big numbers!  Indeed, most people set their "correctness checks" to have a tolerance of 1e-4 in relative error.  Only 3 of 40 of these errors would satisfy the derivative check!

In numerical differentiation, we typically declare that our gradient code has an error if the difference between the midpoint finite-difference approximation and our gradient code differ by more than 1e-4 in relative error.  But even for simple neural networks, I can construct a situation where this difference happens by refactoring the code.  

xxx neural net example xxx

Both of these algorithms are correct, but yet their differences are extreme even in small examples.

The question remains: is this really a big deal?  Even in the 80s and 90s, engineers showed that SGD would converge if you only get the MSB correct in your gradient code.  New work at Deepmind suggests that "synthetic gradients" which have little to do with the true SGD direction can still give speedups.  So maybe this is no big deal?  The only reason it's a big deal is that it makes debugging hard, and you should be aware.
