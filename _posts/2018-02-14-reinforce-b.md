---
layout:     post
title:      The Policy of Truth
date:       2018-02-14 0:00:00
summary:    An outsider tour of reinforcement learning, Part 7. Policy gradient doesn't have gradients.
author:     Ben Recht
visible:    false
---

Our first generic candidate for solving reinforcement learning is _policy gradient_. Policy gradient is an incredibly attractive algorithm as it apparently lets one fine tune a program to solve any problem without any domain knowledge. Of course, anything that makes such a claim must be too general for its own good. Indeed, if you dive into it, **policy gradient is nothing more than random search dressed up in mathematical symbols and lingo**.

I apologize in advance that this is one of the more notation heavy posts. Policy gradient makes excessive use of notation to fool you into thinking there is something deeper going on than there really is. See if you can find the places where sleight of hand occurs.

Let's start with the super general problem that people solve with policy gradient. Recall that a _trajectory_ is a sequence of states and control actions generated by a dynamical system.

$$
\tau_t = (u_1,…,u_{t-1},x_0,…,x_t) \,.
$$

A _control policy_ (or simply “a policy”) is a function, $\pi$, that takes a trajectory from a dynamical system and outputs a new control action.  Note that $\pi$ only gets access to previous states and control actions. Our goal remains to find a policy that maximizes the total reward after $L$ time steps.

In policy gradient, we fix our attention on _parametric, randomized policies_.  The policy $\pi$ has a list of parameters to tune, $\vartheta$. And rather than returning a specific control action, we assume that $\pi$ is a probability distribution over actions. An action is chosen in practice in each step by _sampling_ from this distribution $\pi$. You might ask, why are we sampling? That's a great question! But let's not get bogged down by reasonable questions and press on.

Let's write $\pi_\vartheta$ to max explicit the dependence on the parameters $\vartheta$. Since $\pi_\vartheta$ is a probability distribution, using $\pi_\vartheta$ as a policy induces a probability distribution over trajectories:

$$
	\mathbb{P}(\tau;\vartheta) = \prod_{t=0}^{L-1} \mathbb{P}(x_{t+1}|x_{t},u_{t}) \pi_\vartheta(u_t|\tau_t)\,.
$$

Moreover, we can overload notation and indefinite

$$
	R[\tau] = \sum_{t=0}^N R_t[x_t,u_t]
$$

Then our optimization problem for reinforcement learning tidily becomes:

$$
\begin{array}{ll}
\mbox{maximize}_{\vartheta} & \mathbb{E}_{p(\tau|\vartheta)}[ R[\tau]]
\end{array}
$$

We can make this even cleaner by defining

$$
	J(\vartheta) := \mathbb{E}_{p(\tau \vert \vartheta)}[ R[\tau]]\,.
$$

Our goal in reinforcement learning can now be even more compactly written as

$$
\begin{array}{ll}
\mbox{maximize}_{\vartheta} & J(\vartheta)\,.
\end{array}
$$

## The REINFORCE algorithm

Having set up the problem in tidy notation, Policy Gradient can now be derived by the following clever trick:

$$
\begin{align*}
	\nabla J(\vartheta) &= \int R[\tau] \nabla p(\tau;\vartheta) d\tau\\
	&= \int R[\tau] \left(\frac{\nabla p(\tau;\vartheta)}{p(\tau;\vartheta)}\right) p(\tau;\vartheta) d\tau\\
	&= \int \left( R[\tau] \nabla \log p(\tau;\vartheta) \right) p(\tau;\vartheta)d\tau	\\
  &= \mathbb{E}_{p(\tau;\vartheta)}\left[ R[\tau] \nabla \log p(\tau;\vartheta) \right]\,.
\end{align*}
$$

This calculation reveals that the gradient of $J$ with respect to $\vartheta$ is the expected value of the function

$$
	G(\tau,\vartheta) = R[\tau] \nabla \log p(\tau;\vartheta)
$$

Hence, if we sample a trajectory $\tau$ by running policy $\pi_\vartheta$, we can compute $G(\tau,\vartheta)$ and will have an unbiased estimate of the gradient of $J$. We can follow this direction and will be running stochastic gradient descent on $J$.

What is more magic, is that the function $G(\tau,\vartheta)$ can be computed without knowing the equations that govern the dynamical system. To see this note that

$$
	\mathbb{P}(x_{t+1}|x_{t},u_{t})
$$

is _not_ a function of the parameter $\vartheta$. Hence,

$$
	\nabla_\vartheta \log \mathbb{P}(\tau;\vartheta) = \sum_{t=0}^{L-1} \nabla_\vartheta \log \pi_\vartheta(u_t|\tau_t)\,.
$$
These derivatives can be computed provided that $pi_\vartheta$ is differentiable and you have the latest version of [autograd](xxx) installed.

To sum up, we have a fairly miraculous method that lets us optimize an optimal control problem without knowing anything about the dynamics of the system.

1. Choose some initial guess $\vartheta_0$ and stepsize sequence $\{\alpha_k\}$. Set $k=0$.
2. Sample $\tau_k$ by running the simulator with policy $\pi_{\vartheta_k}$.
3. Set $\vartheta_{k+1} = \vartheta_k + \alpha_k R(u_k) \sum_{t=0}^{L-1} \nabla_\vartheta \log \pi_\vartheta(u_t\vert \tau_t)$.
4. Increment $k=k+1$ and go to step 2.

The main appeal of policy gradient is that it is this easy. If you can efficiently sample from $\pi_\vartheta$, you can run this algorithm on essentially any problem. You can fly quadcopters, you can cool data centers, you can teach robots to open doors. The question becomes, of course, can you do this well? I think that a simple appeal to the Linearization Principle will make it clear that Policy Gradient is almost never an algorithm that you'd want to use.

## Why are we using probabilistic policies again?

Before talking about linear models, let's step back and consider a pure optimization setup. We added a bunch of notation to reinforcement learning so that at the end, it seemed like we were only aiming to maximize an unconstrained function.  Let's remove all of the dynamics and consider the _one step_ optimal control problem. Given a function $R[u]$, I want to find the $u$ that makes this as large as possible. That is, I'd like to solve the optimization problem

$$
\begin{array}{ll}
	\mbox{maximize}_u & R[u] \,.
	\end{array}
$$


Now, bear with me for a second into a digression that might seem tangential. Any optimization problem like this is equivalent to an optimization over probability distributions on $u$.

$$
\begin{array}{ll}
	\mbox{maximize}_{p(u)} & \mathbb{E}_p[R[u]]
\end{array}
$$

The equivalence is really straight forward: if $u_\star$ is the optimal solution, then you'll get the same cost if you put a Delta-function around $u_\star$.  Moreover, if $p$ is a probability distribution, it's clear that the _expected reward_ can never be larger than maximal reward achievable by a fixed $u$. So I can either optimize over $u$ or I can optimize over _distributions_ over $u$.

Now here is where the first sleight of hand often occurs in Policy Gradient. Rather than optimizing over the space of of all probability distributions, we optimize over a parametric family $p(u;\vartheta)$. For example, we could restrict our attention to Gaussian distributions or some other model which is easy to parameterize and to sample from (the sampling part is essential as we have already seen). If this family contains all of the delta functions, then the optimal values still coincide. But, as in the case of Gaussians, if they don't contain the delta functions, you will only get an upper bound on the optimal cost no matter how good of a probability distribution you find. As a result, if you sample $u$ from the policy, their expected reward will necessarily be suboptimal.

Note that there is no need for a randomized policy in the basic optimal control problem we have been studying. And there is certainly no need for the simple LQR problem. The probabilistic policy is a modeling choice, and one that is never better than a deterministic policy.

## The REINFORCE algorithm

Note that the policy gradient algorithm is a general purpose method for finding stochastic gradients of costs of the form

$$
\begin{array}{ll}
	\mbox{maximize}_{\vartheta} & J(\vartheta):=\mathbb{E}_{p(u;\vartheta)}[R[u]]
	\end{array}
$$

The log-likelihood trick works in full generality:

$$
\begin{align*}
	\nabla J(\vartheta) &= \int R(u) \nabla p(u;\vartheta) du\\
	&= \int R(u) \left(\frac{\nabla p(u;\vartheta)}{p(u;\vartheta)}\right) p(u;\vartheta) du\\
	&= \int \left( R(u) \nabla \log p(u;\vartheta) \right) p(u;\vartheta)du	\\
  &= \mathbb{E}_{p(u;\vartheta)}\left[ R(u) \nabla \log p(u;\vartheta) \right]\,.
\end{align*}
$$

Hence the REINFORCE algorithm is a general purpose algorithm:

1. Choose some initial guess $\vartheta_0$ and stepsize sequence $\{\alpha_k\}$. Set $k=0$.
2. Sample $u_k$ i.i.d., from $p(u;\vartheta_k)$.
3. Set $\vartheta_{k+1} = \vartheta_k + \alpha_k R(u_k) \nabla \log p(u_k;\vartheta_k)$.
4. Increment $k=k+1$ and go to step 2.

This seems weird: we get a stochastic gradient, but the function we cared about optimizing---$R$---is only accessed through function evaluations. We never compute gradients of $R$ itself. So is this algorithm any good?

It depends on what you are looking for. If you're looking for something to compete with gradients, no. It's a terrible algorithm. If you're looking for an algorithm to compete with a finite difference approximation to $R$ then... it's still a terrible algorithm. But the math is cute.

The thing is, the Linearization Principle suggests this is algorithm should be discarded almost immediately. Let's consider the most trivial example of LQR:
$$
	R[u] = -||u-z||^2
$$
Let $p(u;\vartheta)$ be a multivariate Gaussian with mean $\vartheta$ and variance $\sigma^2 I$.  What does policy gradient do?  First, note that

$$
	\mathbb{E}_{p(u;\vartheta)} = -\|\vartheta-z\|^2 - \sigma^2 d
$$

Obviously, the best thing to do would be to set $\vartheta=z$. Note that the expected cost is off by $\sigma^2 d$ at this point, but at least this would be finding a good guess for $u$.  Also, as a function of $\vartheta$, $J$ is _strongly convex_, and the most important thing to know is the expected norm of the gradient as this will control the number of iterations. Now, if you start at $\vartheta=0$, then the norm of the gradient is

$$
	g=\frac{||z||^2 \omega_0}{\sigma^2}
$$

And the norm is $d \|z\|^2$. That's actually quite large! The norm of the gradient is proportional to $d$.

Many people have analyzed the complexity of this method, and it is indeed not great. If the function values are noisy, even for convex functions, the convergence rate is $O((d^2/T)^{-1/3})$, and this assumes you get the algorithm parameters exactly right. For strongly convex functions, you can possibly eke out a decent solution in $O((d^2/T)^{-1/2})$ function evaluations.

Note that matters only get worse as we bring in dynamics. The policy gradient update for LQR is very noisy, and its variance grows with the simulation length $L$. Moreover, the search for $\vartheta$ is necessarily nonconvex if one is searching for a simple static policy. While this could work in practice, we already have so many hurdles in our face, that it suggests we should look for an alternative.


## What is going on?

I find it shocking that Policy Gradient wasn't ruled out as a bad idea in 1992. My guess is that there was a lot less implementing then and the mathematics [looks so appealing on its own](http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) that it's easy to lose sight of what would happen if the method actually got coded up.

Lots of papers have been applying policy gradient to all sorts of different settings, and claiming crazy results, but it's clear that they are just dressing up pure random search in a clever outfit. When you end up with a bunch of papers showing that genetic algorithms are competitive with your methods, this does not mean that we’ve made an advance in genetic algorithms. It is far more likely that this means that your method is a lousy implementation of pure random search.

Regardless, both genetic algorithms and policy gradient require an absurd number of samples. This is OK if you are willing to spend millions of dollars on AWS and never actually want to tune a physical system. But there must be a better way.

I don’t think I can overemphasize the point that policy gradient and RL are not magic. I’d go as far as to say that policy gradient and its derivatives are legitimately bad algorithms. In order to make them work well, you need lots of tricks. [Algorithms which are hard to tune, hard to reproduce](xxx), and [don’t outperform off the shelf genetic algorithms](xxx) are bad algorithms.

We’ll come back to this many times in this series: for any application where policy gradient is successful, a dramatically simpler and more robust algorithm exists that will match or outperform it. It’s never a good idea, and I cannot for the life of me figure out why it is so popular.

Indeed! Let’s turn back to LQR and look at some other strategies that might be more successful than policy gradient.
