---
layout:     post
title:      Lessons from Optics, the other Deep Learning
date:       2018-01-25 7:00:00
summary:    How deep learning could to be more like optics
author:     Ali Rahimi
visible:    false
---

I'll start with a question: 
Would you say deep learning is mature enough to be taught in high schools?

Here's why I ask. Some time ago, I received an email from a product manager at a
very large company. I love publicly sharing personal emails, so I'll post it
here.

{: .center}
![BUT HER EMAILS](/assets/optics/email.png){:width="300px"}

I sat on this email for days. I never came up with a constructive answer.

If anything, I wanted reply that maybe her engineers *should* be scared.

{: .center}
![Pipelines. The source of all prosperity and misery](/assets/optics/vgg.png)

Imagine you’re an engineer, you’re given this net, and you’re asked to make it
work better on a dataset. You might presume each of these layers is there for
a reason. But as a field, we don’t yet have a common language to express these
reasons. The way we teach deep learning is very different from the
way we teach other technical disciplines.

A few years ago, I got into optics. In optics, you also build stacks of
components that process inputs as a pipeline. Here’s a camera lens.

{: .center}
![My eyes hurt](/assets/optics/lens.png){:width="460px"}

To design something like this, you start with basic stackup, usually named after
the famous person who invented it. You simulate the stackup to determine how it
falls short of your requirements, then insert some components to correct the
shortcomings.

You run this system through a numerical optimizer to tune its parameters, like
the shape of the surfaces, their locations, and their inclinations, to maximize
some design objective. Then you simulate, modify, optimize, and repeat.

Just like with a deep net.

Each of the 36 elements in the stackup is deliberately inserted to correct an
aberration of some kind. This requires a very clear mental model for what each
component does to the light that passes through it. That mental model is usually
in terms of an action on light, like refraction, reflection, diffraction,
dispersion, or wavefront correction.

{: .center}
![refraction](/assets/optics/refraction.gif){:width="110px" height="90px"}
![reflection](/assets/optics/reflection.jpg){:width="110px" height="90px"}
![diffraction](/assets/optics/diffraction.jpg){:width="110px" height="90px"}
![dispersion](/assets/optics/dispersion.jpg){:width="110px" height="90px"}
![wavefront correction](/assets/optics/wavefront-correction.png){:width="110px" height="90px"}

People aren’t scared of this design process. Each year, the US graduates a thousand
optical engineers who go on to design lenses that work. And they're not scared
to go to work.

That’s not because optics is easy. I think it’s because the mental models
in optics are organized well.

Modern optics is taught in layers of abstraction.

{: .center}
![Absractions all the way down](/assets/optics/abstractions.png){:width="250px"}

At the simplest layer, at the top, is ray optics. Ray optics is a simplification
of wave optics. The rays represent the normal vectors of the wavefronts in wave
optics. Wave optics is an approximate solution to Maxwell’s equations. And Maxwell’s
equations can be derived from quantum physics, which I don’t actually
understand.

Each layer is derived from the layer below it, by making simplifying
assumptions.

{: .center}
![Alchemy up top, science at the bottom](/assets/optics/optics-scorecard.png){:width="650px"}

As such, each layer can explain a more sophisticated set of phenomena than the layer
above it. I spend most of my time designing systems at the top four levels of
abstraction.

This is how we teach optics today.  But these theories weren’t always organized
in a stack like this. Until a hundred years ago, some of these theories co-existed
in a conflicting state.  Practitioners had mostly folkloric theories of these
phenomena.

{: .center}
![And yet it magnifies](/assets/optics/galileo.jpg){:width="250px"}

That didn’t stop Galileo from building a pretty good telescope nearly a hundred
years before Newton formalized ray optics. Galileo had good enough mental model
of light to build a telescope that could magnify 10x. But his understanding
wasn’t good enough to correct chromatic aberrations or to get a wide field of view.

Before these theories were unified, they each had to start with a fundamental
theory of light from the ground up, and so make their own unrealistic assumptions.
Newton’s ray optics modeled light as a spray of particles that could either be
attracted to or repelled from solid matter.  Huygens modeled light
as a longitudinal pressure wave through a mystical medium called “ether”. He
modeled light as sound.  Maxwell also assumed light propagates through ether. You
can still see the remnant of this in the names of the coefficients in Maxwell’s
equations.


## Silly Models, yes. But quantitative, and Predictive.

As silly as these assumptions may sound today, these models were quantitative, and
they had predictive power. You could plug numbers into them, and get numerical
predictions out. That’s tremendously useful to an engineer.


## Seeking: A modular language to describe the action of each layer.

It would be easier to design deep nets if we could talk about the action of
each of its layers the way we talk about the action of an optical element
on the light that passes through it.

We might talk about convolutional layers as running matched filters against their
inputs, and the ensuing nonlinearities as pooling. This is a relatively low-level
description, akin to describing the action of a lens in terms of quantum
effects.

Maybe there are higher level abstractions of these layers, in terms of a
quantity that is modified as it passes through the layers, akin to the action
of lens in terms of how it bends rays.

And it would be nice if this abstraction were quantitative, so you could
plug numbers into some formula and run back-of-the-envelope analyses to help
you design your network.


## We're far from this kind of language. Let's start with something simpler.

But I'm getting carried away with fantasies.

Let's start with something simpler. We already have mental models of how
training deep nets works. Let's try to organize them these into a theory,
and see what phenomena these theories explain. I’ve collected a sampling
phenomena worth explaining. Let's see how our theories do.

Before I get too far, I admit that this exercise is doomed. Optics had 300 years
to do this. I spent a Saturday afternoon on this. In exchange I'm just delivering
my findings in a blog post.


#### _Phenomenon:_ A random initializer for SGD is good enough. But small numerical errors thereafter, or bad step sizes, break SGD.

Some practitioners have observed that small changes in how gradients are
accumulated can result in vastly different performance on the test set. This
comes up for example, when you train with a GPU instead of a CPU
([1](https://github.com/tensorflow/tensorflow/issues/2226),
[2](https://github.com/tensorflow/tensorflow/issues/2732)).

Do you think this is a legitimate observation that's worth explaining?

Or do you think this a spurious observation, and it's probably not true?

Or maybe you think there is something wrong with this observation. Maybe its
logically self-contradictory in some way, or it's a malformed claim?

There are several reactions, but let me record this as a phenomenon and
move on.


#### _Phenomenon:_ Shallow local minima generalize better than sharp ones.

This claim is currently very fashionable. Some people think this is probably true
([3](https://arxiv.org/abs/1609.04836), [4](https://arxiv.org/abs/1611.01838),
[5](https://arxiv.org/abs/1704.04289), [6](https://arxiv.org/abs/1710.06451)), and others,
including myself, believe that it can't be true on logical grounds.
Others have refuted the claim on empirical grounds
([7](https://arxiv.org/abs/1703.04933)). Yet others have offered refined variants
of this claim ([8](https://arxiv.org/abs/1706.08947)), but confusion
remains ([9](https://twitter.com/beenwrekt/status/941005520420225025)).

I'll note that this phenomenon might be disputed, but record it nevertheless.


#### _Phenomenon:_ Inserting batch norm layers speeds up SGD.

That batchnorm works is a mostly undisputed claim (I offer only one
redoubtable exception ([10](http://nyus.joshuawise.com/batchnorm.pdf))).

I'll record this as a phenomenon with no reservation.


#### _Phenomenon:_ SGD is successful despite many local optima and saddle points.

There are several claims packed in this one.  It is often claimed that the training
loss surface of deep learning is rife with saddle points and local 
minima ([11](https://arxiv.org/abs/1712.04741)). In addition, it is variously
claimed either that gradient descent can traverse these hazards
([12](https://arxiv.org/abs/1412.6544)), or that it need not traverse these hazards to
produce a solution that generalizes well
([13](https://arxiv.org/abs/1712.04741)).
It is also just as often claimed that the loss surface of deep models is altogether benign
([14](http://openaccess.thecvf.com/content_cvpr_2017/html/Haeffele_Global_Optimality_in_CVPR_2017_paper.html)).

I'll record this a phenomenon, begrudgingly.


#### _Phenomenon:_ Dropout works better than other randomization strategies.

I don't know how to categorize the class of dropout-like things, so I'm calling
them "randomization strategies" here.

Recorded with apologies and without discussion.


#### _Phenomenon:_ Deep nets can memorize random labels, and yet they generalize.

The evidence is plain ([15](https://arxiv.org/abs/1611.03530)), 
witnessed and supported by some dear friends.

Recorded as a phenomenon without further discussion, despite the conflict of interest, 


## Explications 

We've gathered some phenomena. Let's see what theories can explain them away.

From the papers I cited above, I've gathered the theories that I think have the
best chance of explaining these phenomena.

Let's see how we fare:

{: .center}
![You're fired](/assets/optics/deeplearning-scorecard.png){:width="650px"}

There are a few reasons to not get excited.

First, I couldn't organize the theories into a hierarchy of
abstractions. What made optics easy to learn isn't at all manifest here.

Second, we couldn't agree on many of the observations that we were trying to
explain away.

Third, I suspect some of the theories I've gathered aren't true.


## My Point

There’s a mass influx of newcomers to our field and we’re equipping them with
little more than folklore and pre-trained deep nets, then asking them to innovate. 
We can barely agree on the phenomena that we should be explaining away. I think 
we’re far from teaching this stuff in high schools.

How do we get there?

It would be nice if we could provide mental models, at various layers of
abstraction, of the action of the layers of a deep net. What could be our
equivalent of refraction, dispersion, and diffraction? Maybe you already think
in terms of these actions, but we just haven't standardized our language
around these concepts?

We ought to be able to gather a set of phenomena that we all agree on. We could
then try to explain them away. What is our equivalent of Newton Rings, the
Kerr effect, or the Faraday effect?

With a small group of colleagues, we've started a massive empirical effort to catalogue mental models
that are ambient in our field, to formalize them, and to then validate them with
experiments. It’s a lot of work. I think it’s the first step to developing a
hierarchical mental model of deep learning that you could teach in high schools.
If you want to chat about this project or to help out,
please email me at `ali@leavemeout.com`. Comments welcome on 
[Twitter](linktotweet).
