<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Off the convex path</title>
    <description>Algorithms off the convex path.</description>
    <link>http://offconvex.github.io/</link>
    <atom:link href="http://offconvex.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
     
      <item>
        <title>Nature, Dynamical Systems and Optimization</title>
        <description>&lt;p&gt;The language of dynamical systems is the preferred choice of scientists to model a wide variety of phenomena in nature. The reason is  that, often, it is easy to  &lt;em&gt;locally&lt;/em&gt; observe or understand what happens to a system in one time-step. Could we then piece this local information together to make deductions about the  &lt;em&gt;global&lt;/em&gt; behavior of these dynamical systems? The hope is to understand some of nature’s algorithms and, in this quest, unveil new algorithmic techniques. In this first of a series of posts, we give a gentle introduction to dynamical systems and explain what it means to view them from the point of view of optimization.&lt;/p&gt;

&lt;h2 id=&quot;dynamical-systems-and-the-fate-of-trajectories&quot;&gt;Dynamical Systems and the Fate of Trajectories&lt;/h2&gt;
&lt;p&gt;Given a system whose state at time $t$ takes a value $x(t)$ from a domain $\Omega,$ a dynamical system over $\Omega$ is a function $f$ that describes how this state evolves: one can write the update as 
[ \frac{dx(t)}{dt} = f(x(t))    \ \ \  \mathrm{or} \ \ \ x(t+1)=x(t) + f(x(t))] 
 in continuous or discrete time respectively. In other words, $f$ describes  what happens  in one unit of time to each point in the domain $\Omega.$ Classically, to study a dynamical system is to study  the eventual fate of its &lt;em&gt;trajectories&lt;/em&gt;, i.e., the paths traced by successive states of the system starting from a given state. For this question to make sense, $f$ must not take any state out of the domain. However,  a priori, there is nothing to say that $x(t)$ remains in $\Omega$ beyond $x(0).$  This is the problem of  &lt;em&gt;global existence&lt;/em&gt; of trajectories and can sometimes be quite hard  to establish. Assuming that the dynamical system at hand has a solution for all times   for all starting points, and $\Omega$ is compact, the trajectories either tend to  &lt;em&gt;fixed points&lt;/em&gt;, &lt;em&gt;limit cycles&lt;/em&gt; or end up in  &lt;em&gt;chaos&lt;/em&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/trajectories.jpg&quot; alt=&quot;The fate of trajectories&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;A fixed point of a dynamical system, as the name suggests, is a state $x \in \Omega$ which does not change on the application of $f$, i.e.,  $f(x)=0.$  A  fixed point is said to be &lt;em&gt;stable&lt;/em&gt; if trajectories starting at &lt;em&gt;all&lt;/em&gt; nearby points eventually converge to it and &lt;em&gt;unstable&lt;/em&gt; otherwise. Stability is a property that one might expect to find in nature. Limit cycles are closed trajectories with a similar notion of stability/unstability, while limits of trajectories which are neither fixed points or limit cycles are (loosely) termed as chaos.&lt;/p&gt;

&lt;h2 id=&quot;what-do-dynamical-systems-optimize&quot;&gt;What do Dynamical Systems Optimize?&lt;/h2&gt;

&lt;p&gt;For now, we will consider the class of dynamical systems which only have fixed points, possibly many. In this setting, one can define a function $F$ which maps an  $x \in \Omega$  to its limit under the repeated application of $f.$ Note that to make this function well-defined we might have to look at the closure of $\Omega.$  This brings us to  the following broad,  admittedly not well-defined and widely open question that we would like to study:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Given a dynamical system $(\Omega,f)$, what is  $F$?&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When $f$ happens to be the negative gradient of  a convex function $g$ over some convex domain $\Omega,$ the dynamical system $(\Omega,f)$ is nothing but an implementation of gradient descent to find the minimum of $g$, answering our question perfectly.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, in many cases, $f$ may not be a gradient system and understanding what $f$ optimizes may be quite difficult. The fact that there may be multiple  fixed-points necessarily means that trajectories starting at different points  may converge to different points in the domain– giving us a sense of non-convexity. In such cases, answering our question can be a daunting task and, currently, there is no general theory for it. We present two dynamical systems from nature – one easy and one not 	quite.&lt;/p&gt;

&lt;h2 id=&quot;evolution-and-the-largest-eigenvector&quot;&gt;Evolution and the Largest Eigenvector&lt;/h2&gt;

&lt;p&gt;As a simple but important example, consider a population consisting of $n$-types which is subject to the forces of evolution and held at a constant size, say one unit  of mass. Thus, if we let $x_i(t)$ denote the fraction of type $i$ at time $t$ in the population, the domain becomes 
$\Delta^n=\{x \in \mathbb{R}^n_{&amp;gt;0}: x \geq 0 \; \mathrm{and} \; \; \sum_i x_i=1 \},$  the unit simplex. 
The update  function is&lt;br /&gt;
[ f(x)= Qx - \Vert Qx \Vert_1 \cdot x ]
for a positive matrix  $Q \in \mathbb{R}_{&amp;gt;0}^{n \times n}.$ 
The properties of a natural environment in which the population is evolving can be captured by a matrix $Q,$  see this &lt;a href=&quot;https://books.google.ch/books?id=YXrIRDuAbE0C&amp;amp;hl=en&quot;&gt;textbook&lt;/a&gt; which is dedicated to the study of such dynamical systems. Mathematically, to start with, note that $f$ maps any point in the simplex to a point in the simplex.  Thus, starting at any point in $\Delta^n,$ the trajectory remains in $\Delta^n.$ What are the fixed points of $f$? These are vectors $x \in \Delta^n$ such that $Qx=\Vert Qx \Vert_1 \cdot x$ or the eigenvectors of $Q.$ Since $Q&amp;gt;0,$ the &lt;a href=&quot;https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem&quot;&gt;Perron-Frobenius theorem&lt;/a&gt;  tells us that $Q$ has unique  eigenvector $v \in \Delta^n$ and, starting at any $x(0) \in \Delta^n,$ $x(t) \rightarrow v$ as $t \rightarrow \infty.$ Thus, in this case, simple linear algebra can allow us to deduce that $f$ has exactly one fixed point and, thus, we can answer what is $f$ achieving globally: $f$ is nothing but nature’s implementation of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_iteration&quot;&gt;Power Method&lt;/a&gt; to compute the maximum eigenvector of $Q$! Biologically, the  corresponding eigenvalue can be shown to be the &lt;em&gt;average fitness&lt;/em&gt; of the population which is what nature is trying to maximize. It may be worthwhile to note that the maximum eigenvalue problem is non-convex as such.&lt;/p&gt;

&lt;h2 id=&quot;solving-linear-programs-by-molds&quot;&gt;Solving Linear Programs by Molds?&lt;/h2&gt;

&lt;p&gt;Let us conclude with an interesting dynamical system inspired by the inner workings of a &lt;a href=&quot;https://www.youtube.com/watch?v=czk4xgdhdY4&quot;&gt;slime mold&lt;/a&gt;; see &lt;a href=&quot;http://link.springer.com/chapter/10.1007%2F978-3-642-30870-3_35&quot;&gt;here&lt;/a&gt; for a discussion on how this class of dynamics was discovered. Suppose $A \in \mathbb{R}^{n \times m}$ is a matrix and $b \in \mathbb{R}^n$ is a vector. The domain is the positive orthant $\Omega = \{x \in \mathbb{R}{^m}: x&amp;gt;0 \}.$ For a point $x \in \mathbb{R}^m,$ let $X$ denote the diagonal matrix such that $X_{ii}=x_i.$  The evolution function  is then:
[ \frac{dx}{dt} = X ( A^\top (AXA^\top)^{-1} b - \vec{1}), ]
where $\vec{1}$ is the vector of all ones.  Now the problem of existence of a solution is neither trivial nor can be ignored as, for the dynamical system to make sense, $x$ has to be positive. Further, it can be argued in a formal sense that this dynamical system is not a gradient descent. What then can we say about the trajectories of this dynamical system? As it turns out, it can be &lt;a href=&quot;http://arxiv.org/abs/1511.07020&quot;&gt;shown&lt;/a&gt; that starting at any $x&amp;gt;0,$ the dynamical system is a gradient descent on a natural Riemannian manifold and converges to a unique point among the solutions to the following linear program,
[ \min \; \sum_i x_i \ \ \  \mathrm{s.t.} \ \ \ Ax=b, \ \ x \geq 0, ] 
which gives us a new algorithm for linear programming. We will explain how in a subsequent post.&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 02:09:00 -0800</pubDate>
        <link>http://offconvex.github.io/2015/12/20/dynamical-systems-1/</link>
        <guid isPermaLink="true">http://offconvex.github.io/2015/12/20/dynamical-systems-1/</guid>
      </item>
     
    
     
    
     
      <item>
        <title>Tensor Methods in Machine Learning</title>
        <description>&lt;p&gt;&lt;em&gt;Tensors&lt;/em&gt; are high dimensional generalizations of matrices. In recent years &lt;em&gt;tensor decompositions&lt;/em&gt; were used to design learning algorithms for estimating parameters of latent variable models like Hidden Markov Model, Mixture of Gaussians and Latent Dirichlet Allocation (many of these works were considered as examples of “spectral learning”, read on to find out why). In this post I will briefly describe why &lt;em&gt;tensors&lt;/em&gt; are useful in these settings.&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Singular Value Decomposition (SVD)&lt;/a&gt;, we can write a matrix $M \in \mathbb{R}^{n\times m}$ as the sum of many rank one matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = \sum_{i=1}^r \lambda_i \vec{u}_i \vec{v}_i^\top.&lt;/script&gt;

&lt;p&gt;When the &lt;em&gt;rank&lt;/em&gt; $r$ is small, this gives a concise representation for the matrix $M$ (using $(m+n)r$ parameters instead of $mn$). Such decompositions are widely applied in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tensor decomposition&lt;/em&gt; is a generalization of low rank matrix decomposition. Although &lt;a href=&quot;http://arxiv.org/abs/0911.1393&quot;&gt;most tensor problems are NP-hard&lt;/a&gt; in the worst case, several natural subcases of tensor decomposition can be solved in polynomial time. Later we will see that these subcases are still very powerful in learning latent variable models.&lt;/p&gt;

&lt;h2 id=&quot;matrix-decompositions&quot;&gt;Matrix Decompositions&lt;/h2&gt;

&lt;p&gt;Before talking about tensors, let us first see an example of how matrix factorization can be used to learn latent variable models. In 1904, psychologist Charles Spearman tried to understand whether human intelligence is a composite of different types of measureable intelligence.  Let’s describe a highly simplified version of his method, where the hypothesis is that there are exactly two kinds of intelligence: &lt;em&gt;quantitative&lt;/em&gt; and &lt;em&gt;verbal&lt;/em&gt;. Spearman’s method consisted of making his subjects take several different kinds of tests. 
Let’s name these tests &lt;em&gt;Classics, Math, Music&lt;/em&gt;, etc. The subjects scores can be represented by a &lt;em&gt;matrix&lt;/em&gt; $M$, which has one row per student, and one column per test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/matrix.png&quot; alt=&quot;matrix M&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simplified version of Spearman’s hypothesis is that each student has different amounts of quantitative and verbal intelligence, say  $x_{quant}$ and $x_{verb}$ respectively. Each test measures a different mix of intelligences, so say it gives a &lt;em&gt;weighting&lt;/em&gt; $y_{quant}$ to quantitative and $y_{verb}$ to verbal. Intuitively, a student with higher strength on verbal intelligence should perform better on a test that has a high weight on verbal intelligence. Let’s describe this relationship as a simple &lt;em&gt;bilinear&lt;/em&gt; function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score = x_{quant} \times y_{quant} + x_{verb}\times y_{verb}.&lt;/script&gt;

&lt;p&gt;Denoting by $\vec x_{verb}, \vec x_{quant}$ the vectors describing the strengths of the students, and letting $\vec y_{verb}, \vec y_{quant}$ be the vectors that describe the weighting of intelligences in the different tests, we can express matrix $M$ as the sum of two &lt;strong&gt;rank 1&lt;/strong&gt; matrices (in other words, $M$ has rank at most $2$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = \vec x_{quant} \vec y_{quant}^\top + \vec x_{verb} \vec y_{verb}^\top.&lt;/script&gt;

&lt;p&gt;Thus verifying that $M$ has rank $2$ (or that it is very close to a rank $2$ matrix) should let us conclude that there are indeed two kinds of intelligence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/decomposition1.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that this decomposition is not the &lt;em&gt;Singular Value Decomposition&lt;/em&gt; (SVD). SVD requires strong orthogonality constraints (which translates to “different intelligences are completely uncorrelated”) that are not plausible in this setting.&lt;/p&gt;

&lt;h2 id=&quot;the-ambiguity&quot;&gt;The Ambiguity&lt;/h2&gt;

&lt;p&gt;But ideally one would like to take the above idea further: we would like to assign a definitive quantitative/verbal intelligence score to each student. This seems simple at first sight: just read off the score from the decomposition. For instance, it shows Alice is strongest in quantitative intelligence.&lt;/p&gt;

&lt;p&gt;However, this is incorrect, because the decomposition is &lt;strong&gt;not&lt;/strong&gt; unique! The following is another valid decomposition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/decomposition2.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to this decomposition, Bob is strongest in quantitative intelligence, not Alice. Both decompositions explain the data perfectly and we &lt;strong&gt;cannot&lt;/strong&gt; decide &lt;em&gt;a priori&lt;/em&gt; which is correct.&lt;/p&gt;

&lt;p&gt;Sometimes we can hope to find the unique solution by imposing additional constraints on the decomposition, such as all matrix entries have to be nonnegative. However even after imposing many natural constraints, in general the issue of multiple decompositions will remain.&lt;/p&gt;

&lt;h2 id=&quot;adding-the-3rd-dimension&quot;&gt;Adding the 3rd Dimension&lt;/h2&gt;

&lt;p&gt;Since our current data has multiple explanatory decompositions, we need  more data to learn exactly which explanation is the truth. Assume the strength of the intelligence changes with time: we get better at quantitative tasks at night. Now we can let the (poor) students take the tests twice: once during the day and once at night. The results we get can be represented by two matrices $M_{day}$ and $M_{night}$. But we can also think of this as a three dimensional array of numbers -– a tensor $T$ in $\mathbb{R}^{\sharp students\times \sharp tests\times 2}$.
Here the third axis stands for “day” or “night”. We say the two matrices $M_{day}$ and $M_{night}$ are &lt;em&gt;slices&lt;/em&gt; of the tensor $T$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bimatrix.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let $z_{quant}$ and $z_{verb}$ be the relative strength of the two kinds of intelligence at a particular time (day or night), then the new score can be computed by a &lt;em&gt;trilinear&lt;/em&gt; function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score = x_{quant} \times y_{quant} \times z_{quant} + x_{verb}\times y_{verb} \times z_{verb}.&lt;/script&gt;

&lt;p&gt;Keep in mind that this is the formula for &lt;strong&gt;one entry&lt;/strong&gt; in the tensor: the score of one student, in one test and at a specific time. Who the student is specifies $x_{quant}$ and $x_{verb}$; what the test is specifies weights $y_{quant}$ and $y_{verb}$; when the test takes place specifies $z_{quant}$ and $z_{verb}$.&lt;/p&gt;

&lt;p&gt;Similar to matrices, we can view this as a &lt;strong&gt;rank 2&lt;/strong&gt; decomposition of the tensor $T$. In particular, if we use $\vec x_{quant}, \vec x_{verb}$ to denote the strengths of students, $\vec y_{quant},\vec y_{verb}$ to denote the weights of the tests and $\vec z_{quant}, \vec z_{verb}$ to denote the variations of strengths in time, then we can write the decomposition as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \vec x_{quant}\otimes \vec y_{quant}\otimes \vec z_{quant} + \vec x_{verb}\otimes \vec y_{verb}\otimes \vec z_{verb}.&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tensordecomposition.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we can check that the second matrix decomposition we had is no longer valid: there are no values of $z_{quant}$ and $z_{verb}$ at night that could generate the matrix $M_{night}$. This is not a coincidence. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0024379577900696&quot;&gt;Kruskal 1977&lt;/a&gt; gave sufficient conditions for such decompositions to be unique. When applied to our case it is very simple:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;
The decomposition of tensor $T$ is unique (up to scaling and permutation) if none of the vector pairs $(\vec x_{quant}, \vec x_{verb})$, $(\vec y_{quant},\vec y_{verb})$, $(\vec z_{quant},\vec z_{verb})$ are co-linear.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that of course the decomposition is not truly unique for two reasons. First, the two tensor factors are symmetric, and we need to decide which factor correspond to quantitative intelligence. Second, we can scale the three components $\vec x_{quant}$ ,$\vec y_{quant}$, $\vec z_{quant}$ simultaneously, as long as the product of the three scales is 1. Intuitively this is like using different units to measure the three components. Kruskal’s result showed that these are the only degrees of freedom in the decomposition, and there cannot be a truly distinct decomposition as in the matrix case.&lt;/p&gt;

&lt;h2 id=&quot;finding-the-tensor&quot;&gt;Finding the Tensor&lt;/h2&gt;

&lt;p&gt;In the above example we get a low rank tensor $T$ by gathering more data. In many traditional applications the extra data may be unavailable or hard to get. Luckily, many exciting recent developments show that we can &lt;em&gt;uncover&lt;/em&gt; these special tensor structures even if the original data is not in a tensor form!&lt;/p&gt;

&lt;p&gt;The main idea is to use &lt;em&gt;method of moments&lt;/em&gt; (see a nice &lt;a href=&quot;http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html&quot;&gt;post by Moritz&lt;/a&gt;): estimate lower order correlations of the variables, and hope these lower order correlations have a simple tensor form.&lt;/p&gt;

&lt;p&gt;Consider &lt;em&gt;Hidden Markov Model&lt;/em&gt; as an example. Hidden Markov Models are widely used in analyzing sequential data like speech or text. Here for concreteness we consider a (simplified) model of natural language texts(which is a basic version of the &lt;a href=&quot;http://www.offconvex.org/2015/12/12/word-embeddings-1/&quot;&gt;&lt;em&gt;word embeddings&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In Hidden Markov Model, we observe a sequence of words (a sentence) that is generated by a walk of a &lt;em&gt;hidden&lt;/em&gt; Markov Chain: each word has a hidden topic $h$ (a discrete random variable that specifies whether the current word is talking about “sports” or “politics”); the topic for the next word only depends on the topic of the current word. Each topic specifies a distribution over words. Instead of the topic itself, we observe a random word $x$ drawn from this topic distribution (for example, if the topic is “sports”, we will more likely see words like “score”). The dependencies are usually illustrated by the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hmm.png&quot; alt=&quot;Hidden Markov Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More concretely, to generate a sentence in Hidden Markov Model, we start with some initial &lt;em&gt;topic&lt;/em&gt; $h_1$. This topic will evolve as a Markov Chain to generate the topics for future words $h_2, h_3,…,h_t$. We observe &lt;em&gt;words&lt;/em&gt; $x_1,…,x_t$ from these topics. In particular, word $x_1$ is drawn according to topic $h_1$, word $x_2$ is drawn according to topic $h_2$ and so on.&lt;/p&gt;

&lt;p&gt;Given many sentences that are &lt;em&gt;generated exactly&lt;/em&gt; according to this model, how can we construct a tensor? A natural idea is to compute &lt;em&gt;correlations&lt;/em&gt;: for every triple of words $(i,j,k)$, we count the number of times that these are the first three words of a sentence. Enumerating over $i,j,k$ gives us a three dimensional array (a &lt;em&gt;tensor&lt;/em&gt;) $T$. We can further normalize it by the total number of sentences. After normalization the $(i,j,k)$-th entry of the tensor will be an estimation of the &lt;em&gt;probability&lt;/em&gt; that the first three words are $(i,j,k)$. For simplicity assume we have enough samples and the estimation is accurate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_{i,j,k} = \mbox{Pr}[x_1 = i, x_2=j, x_3=k].&lt;/script&gt;

&lt;p&gt;Why does this tensor have the nice low rank property? The key observation is that if we “fix” (condition on) the topic of the second word $h_2$, it cuts the graph into three parts: one part containing $h_1,x_1$, one part containing $x_2$ and one part containing $h_3,x_3$. These three parts are &lt;strong&gt;independent&lt;/strong&gt; conditioned on $h_2$. In particular, the first three words $x_1,x_2,x_3$ are &lt;strong&gt;independent&lt;/strong&gt; conditioned on the topic of the &lt;strong&gt;second&lt;/strong&gt; word $h_2$. Using this observation we can compute each entry of the tensor as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_{i,j,k} = \sum_{l=1}^n \mbox{Pr}[h_2 = l] \mbox{Pr}[x_1 = i| h_2 = l]\times \mbox{Pr}[x_2 = j| h_2 = l]\times \mbox{Pr}[x_3 = k| h_2 = l].&lt;/script&gt;

&lt;p&gt;Now if we let $\vec x_l$ be a vector whose $i$-th entry is the probability of the first word is $i$, given the topic of the &lt;em&gt;second&lt;/em&gt; word is $l$; let $\vec y_l$ and $\vec z_l$ be similar for the second and third word. We can then write the entire tensor as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \sum_{l=1}^n \mbox{Pr}[h_2 = l] \vec x_l \otimes \vec y_l \otimes \vec z_l.&lt;/script&gt;

&lt;p&gt;This is exactly the &lt;strong&gt;low rank&lt;/strong&gt; form we are looking for! Tensor decomposition allows us to &lt;em&gt;uniquely&lt;/em&gt; identify these components, and further infer the other probabilities we are interested in. For more details see the paper by &lt;a href=&quot;http://arxiv.org/abs/1210.7559&quot;&gt;Anandkumar et al. 2012&lt;/a&gt; (this paper uses the tensor notations, but the original idea appeared in the paper by &lt;a href=&quot;https://projecteuclid.org/euclid.aoap/1151592244&quot;&gt;Mossel and Roch 2006&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;implementing-tensor-decomposition&quot;&gt;Implementing Tensor Decomposition&lt;/h2&gt;

&lt;p&gt;Using method of moments, we can discover nice tensor structures from many problems. The uniqueness of tensor decomposition makes these tensors very useful in learning the parameters of the models. But how do we compute the tensor decompositions?&lt;/p&gt;

&lt;p&gt;In the worst case we have bad news: &lt;a href=&quot;http://arxiv.org/abs/0911.1393&quot;&gt;most tensor problems are NP-hard&lt;/a&gt;. However, in most natural cases, as long as the tensor does &lt;em&gt;not&lt;/em&gt; have &lt;em&gt;too many&lt;/em&gt; components, and the components are &lt;em&gt;not adversarially&lt;/em&gt; chosen, tensor decomposition &lt;strong&gt;can&lt;/strong&gt; be computed in polynomial time! Here we describe the algorithm by Dr. Robert Jenrich (it first appeared in a 1970 working paper by &lt;a href=&quot;http://hbanaszak.mjr.uw.edu.pl/TempTxt/Harshman_1970_Foundations%20of%20PARAFAC%20Procedure%20MOdels%20and%20Conditions%20for%20an%20Expalanatory%20Multimodal%20Factor%20Analysis.pdf&quot;&gt;Harshman&lt;/a&gt;, the version we present here is a more general version by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=173234&quot;&gt;Leurgans, Ross and Abel 1993&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jenrich’s Algorithm &lt;br /&gt;
Input: tensor $T = \sum_{i=1}^r \lambda_i \vec x_i \otimes \vec y_i \otimes \vec z_i$.&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Pick two random vectors $\vec u, \vec v$.&lt;/li&gt;
    &lt;li&gt;Compute $T_\vec u = \sum_{i=1}^n u_i T[:,:,i] = \sum_{i=1}^r \lambda_i (\vec u^\top \vec z_i) \vec x_i \vec y_i^\top$.&lt;/li&gt;
    &lt;li&gt;Compute $T_\vec v = \sum_{i=1}^n v_i T[:,:,i] = \sum_{i=1}^r \lambda_i (\vec v^\top \vec z_i) \vec x_i \vec y_i^\top$.&lt;/li&gt;
    &lt;li&gt;$\vec x_i$’s are eigenvectors of $T_\vec u (T_\vec v)^{+}$, $\vec y_i$’s are eigenvectors of $T_\vec v (T_\vec u)^{+}$.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the algorithm, “$^+$” denotes &lt;em&gt;pseudo-inverse&lt;/em&gt; of a matrix (think of it as inverse if this is not familiar).&lt;/p&gt;

&lt;p&gt;The algorithm looks at weighted &lt;em&gt;slices&lt;/em&gt; of the tensor: a weighted slice is a matrix that is the projection of the tensor along the $z$ direction (similarly if we take a &lt;em&gt;slice&lt;/em&gt; of a matrix $M$, it will be a vector that is equal to $M\vec u$). Because of the low rank structure, all the slices must share matrix decompositions with the &lt;strong&gt;same&lt;/strong&gt; components.&lt;/p&gt;

&lt;p&gt;The main observation of the algorithm is that although a &lt;em&gt;single&lt;/em&gt; matrix can have infinitely many low rank decompositions, &lt;em&gt;two&lt;/em&gt; matrices can only have a &lt;strong&gt;unique&lt;/strong&gt; decomposition if we require them to have the same components. In fact, it is highly unlikely for two &lt;em&gt;arbitrary&lt;/em&gt; matrices to share decompositions with the same components. In the tensor case, because of the low rank structure we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_\vec u = XD_\vec u Y^\top; \quad T_\vec v = XD_\vec v Y^\top,&lt;/script&gt;

&lt;p&gt;where $D_\vec u,D_\vec v$ are diagonal matrices. This is called a &lt;em&gt;simultaneous diagonalization&lt;/em&gt; for $T_\vec u$ and $T_\vec v$. With this structure it is easy to show that $\vec x_i$’s are eigenvectors of $T_\vec u (T_\vec v)^{+} = X D_\vec u D_\vec v^{-1} X^+$. So we can actually compute &lt;em&gt;tensor decompositions&lt;/em&gt; using &lt;em&gt;spectral decompositions&lt;/em&gt; for matrices.&lt;/p&gt;

&lt;p&gt;Many of the earlier works (including &lt;a href=&quot;https://projecteuclid.org/euclid.aoap/1151592244&quot;&gt;Mossel and Roch 2006&lt;/a&gt;) that apply tensor decompositions to learning problems have actually independently &lt;em&gt;rediscovered&lt;/em&gt; this algorithm, and the word “tensor” never appeared in the papers. In fact,  tensor decomposition techniques are traditionally called “spectral learning” since they are seen as derived from SVD.  But now we have other methods to do tensor decompositions that have better theoretical guarantees and practical performances. See the survey by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1655230&quot;&gt;Kolda and Bader 2009&lt;/a&gt; for more discussions.&lt;/p&gt;

&lt;h3 id=&quot;related-links&quot;&gt;Related Links&lt;/h3&gt;

&lt;p&gt;For more examples of using &lt;em&gt;tensor decompositions&lt;/em&gt; to learn latent variable models, see the paper by &lt;a href=&quot;http://arxiv.org/abs/1210.7559&quot;&gt;Anandkumar et al. 2012&lt;/a&gt;. This paper shows that several prior algorithms for learning models such as Hidden Markov Model, Latent Dirichlet Allocation, Mixture of Gaussians and Independent Component Analysis can be interpreted as doing tensor decompositions. The paper also gives a proof that &lt;em&gt;tensor power method&lt;/em&gt; is efficient and robust to noise.&lt;/p&gt;

&lt;p&gt;Recent research focuses on two problems: how to formulate other learning problems as tensor decompositions, and how to compute tensor decompositions under weaker assumptions. Using tensor decompositions, we can learn more models that include &lt;a href=&quot;http://arxiv.org/abs/1302.2684&quot;&gt;community models&lt;/a&gt;, &lt;a href=&quot;http://www.cs.columbia.edu/~mcollins/papers/uai2014-long.pdf&quot;&gt;probabilistic Context-Free-Grammars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1503.00424&quot;&gt;mixture of general Gaussians&lt;/a&gt; and &lt;a href=&quot;http://newport.eecs.uci.edu/anandkumar/pubs/NN_GeneralizationBound.pdf&quot;&gt;two-layer neural networks&lt;/a&gt;. We can also efficiently compute tensor decompositions when the &lt;em&gt;rank&lt;/em&gt; of the tensor is much larger than the dimension (see for example the papers by &lt;a href=&quot;http://arxiv.org/abs/1311.3651&quot;&gt;Bhaskara et al. 2014&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1306.5825&quot;&gt;Goyal et al. 2014&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1504.05287&quot;&gt;Ge and Ma 2015&lt;/a&gt;). There are many other interesting works and open problems, and the list here is by no means complete.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Dec 2015 07:00:00 -0800</pubDate>
        <link>http://offconvex.github.io/2015/12/17/tensor-decompositions/</link>
        <guid isPermaLink="true">http://offconvex.github.io/2015/12/17/tensor-decompositions/</guid>
      </item>
     
    
     
      <item>
        <title>Semantic Word Embeddings</title>
        <description>&lt;p&gt;This post can be seen as an introduction to how nonconvex problems arise
naturally in practice, and also the relative ease with which they are often
solved.&lt;/p&gt;

&lt;p&gt;I will talk about &lt;em&gt;word embeddings&lt;/em&gt;, a geometric way to capture
the “meaning” of a word via a low-dimensional vector. They are useful in
many tasks in Information Retrieval (IR) and Natural Language Processing
(NLP), such as answering search queries or translating from one
language to another.&lt;/p&gt;

&lt;p&gt;You may wonder: how can a 300-dimensional vector capture the many
nuances of word meaning? And what the heck does it mean to “capture meaning?”&lt;/p&gt;

&lt;h2 id=&quot;properties-of-word-embeddings&quot;&gt;Properties of Word Embeddings&lt;/h2&gt;
&lt;p&gt;A simple property of embeddings obtained by all the methods I’ll
describe is &lt;em&gt;cosine similarity&lt;/em&gt;: the  &lt;em&gt;similarity&lt;/em&gt; between two words 
(as rated by humans on a $[-1,1]$ scale) correlates with the &lt;em&gt;cosine&lt;/em&gt;
of the angle between their vectors. To 
give an example, the cosine for &lt;em&gt;milk&lt;/em&gt; and
&lt;em&gt;cow&lt;/em&gt; may be $0.6$, whereas for &lt;em&gt;milk&lt;/em&gt; and
&lt;em&gt;stone&lt;/em&gt; it may be $0.2$, which is roughly the similarity
human subjects assign to them.&lt;/p&gt;

&lt;p&gt;A more interesting property of recent embeddings is that they can solve
&lt;em&gt;analogy&lt;/em&gt; relationships via linear algebra.
For example, the word analogy question
&lt;em&gt;man : woman ::king : ??&lt;/em&gt; can be solved by looking for the
word $w$ such that $v_{king} - v_w$ is most similar to
$v_{man} - v_{woman}$; in other words, minimizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||v_w - v_{king} + v_{man} - v_{woman}||^2.&lt;/script&gt;

&lt;p&gt;This simple idea can solve $75\%$ of analogy questions on some standard testbed. Note that the method is completely unsupervised: it constructs the embeddings using a big (unannotated) text corpus; and receives &lt;em&gt;no training&lt;/em&gt; specific to analogy solving). 
Here is a rendering of this linear algebraic relationship between &lt;em&gt;masculine-feminine&lt;/em&gt; pairs.&lt;/p&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;/assets/analogy-small.jpg&quot; style=&quot;width:400px;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Good embeddings have other properties that will be covered in a future
post. (Also, I can’t resist mentioning  that fMRI-imaging of the brain suggests that word embeddings are related to how the human brain
encodes meaning; see the &lt;a href=&quot;http://www.cs.cmu.edu/~tom/pubs/science2008.pdf&quot;&gt;well-known paper of Mitchell et al.&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;computing-word-embeddings-via-firths-hypothesis&quot;&gt;Computing Word embeddings (via Firth’s Hypothesis)&lt;/h2&gt;

&lt;p&gt;In all methods, the word vector is a succinct representation of the &lt;em&gt;distribution&lt;/em&gt; of other words around this word. That this suffices to capture meaning is asserted by &lt;a href=&quot;https://en.wikipedia.org/wiki/Distributional_semantics&quot;&gt;&lt;em&gt;Firth’s hypothesis&lt;/em&gt;&lt;/a&gt;
from 1957, “&lt;em&gt;You shall know a word by the company it keeps.&lt;/em&gt;”  To give an example, if I
ask you to think of a word that tends to co-occur with &lt;em&gt;cow,
drink, babies, calcium&lt;/em&gt;, you would immediately answer:
&lt;em&gt;milk&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Note that we don’t believe Firth’s hypothesis fully accounts
for all aspects of semantics —understanding new metaphors or jokes, for example,
seems to require other modes of experiencing the real world than simply reading text.&lt;/p&gt;

&lt;p&gt;But Firth’s hypothesis does imply a very simple 
word embedding, albeit a very high-dimensional one.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 1&lt;/em&gt;: Suppose the dictionary has $N$ distinct words (in practice, $N =100,000$). Take a very large text corpus (e.g., Wikipedia) and let $Count_5(w_1, w_2)$ be the number of times $w_1$ and $w_2$ occur within a distance $5$ of each other in the corpus. Then the word embedding for a word $w$ is a vector of dimension $N$, with one coordinate for each dictionary word. The coordinate corresponding to word $w_2$ is $Count_5(w, w_2)$. (Variants of this method involve considering cooccurence of $w$ with various phrases or $n$-tuples.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The obvious problem with Embedding 1 is that it uses
extremely high-dimensional vectors. How can we compress them?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 2&lt;/em&gt;: Do dimension reduction by taking the rank-300
singular value decomposition (SVD) of the above vectors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that for an $N \times N$ matrix $M$ this means finding vectors
$v_1, v_2, \ldots, v_N \in \mathbb{R}^{300}$ that minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{ij} (M_{ij} - v_i \cdot v_j)^2 \qquad (1).&lt;/script&gt;

&lt;p&gt;Using SVD to do dimension reduction seems an obvious idea these days but it
actually is not. After all,  it is unclear &lt;em&gt;a priori&lt;/em&gt; why the
above $N \times N$ matrix of cooccurance counts should be close to a
rank-300 matrix. That this is the case was empirically discovered in the paper on
&lt;a href=&quot;http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf&quot;&gt;&lt;em&gt;Latent Semantic Indexing&lt;/em&gt; or LSI&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Empirically, the method can be improved by replacing the counts by their logarithm, as in &lt;a href=&quot;http://lsa.colorado.edu/papers/plato/plato.annote.html&quot;&gt;Latent Semantic Analysis or LSA&lt;/a&gt;. Other authors claim  square root
is even better than logarithm.  Another interesting empirical fact in the LSA paper is that dimension reduction via SVD not only compresses the embedding but &lt;em&gt;improves&lt;/em&gt; its quality. (Improvement via compression is a familiar phenomenon in machine learning.) In fact they can use word vectors to solve word similarity tasks as well as the average American high schooler.&lt;/p&gt;

&lt;p&gt;A research area called &lt;em&gt;Vector Space Models&lt;/em&gt; (see &lt;a href=&quot;https://www.jair.org/media/2934/live-2934-4846-jair.pdf&quot;&gt;survey by Turney and Pantel&lt;/a&gt;) studies various modifications of the above idea. Embeddings are also known to improve if we reweight the various terms in the above expression (2): popular reweightings include TF-IDF, PMI, Logarithm, etc.&lt;/p&gt;

&lt;p&gt;Let me point out that reweighting the
$(i, j)$ term in expression (1) leads to a &lt;em&gt;weighted&lt;/em&gt;
version of SVD, which is NP-hard. (I always emphasize to my students
that a polynomial-time algorithm to compute rank-k  SVD is a miracle, since 
modifying the problem
statement in small ways makes it NP-hard.) But in practice, weighted SVD can be
solved on a laptop in less than a day —remember, $N$ is rather large, about $10^5$!—
by simple gradient descent on the objective (1), possibly also using
a regularizer. (Of course,  &lt;a href=&quot;http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html&quot;&gt;a lot has been proven about such gradient descent methods in context of convex optimization&lt;/a&gt;; the surprise is that they work also in such nonconvex settings.) Weighted SVD is a subcase of &lt;em&gt;Matrix Factorization&lt;/em&gt; approaches in machine learning, which we will also encounter again in upcoming posts.&lt;/p&gt;

&lt;p&gt;But returning to word embeddings, the following question had not been
raised or debated, to the best of my knowledge: &lt;em&gt;What property of human language explains the
fact that these very high-dimensional matrices derived (in a nonlinear way) from word cooccurences
are close to low-rank matrices?&lt;/em&gt; (In a future blog
post I will describe our new &lt;a href=&quot;http://arxiv.org/abs/1502.03520&quot;&gt;theoretical explanation&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The third embedding method I wish to describe uses 
&lt;em&gt;energy-based models&lt;/em&gt;, for instance the
&lt;a href=&quot;https://code.google.com/p/word2vec/&quot;&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; family of methods&lt;/a&gt; from 2013 by the Google team of Mikolov et al., which also created a buzz due to the above-mentioned linear algebraic method to solve word analogy tasks.
The &lt;strong&gt;word2vec&lt;/strong&gt; models are inspired by pre-existing neural net models for language
(basically, the word embedding corresponds to the neural net’s internal representation of the word; see &lt;a href=&quot;http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/&quot;&gt;this blog&lt;/a&gt;.).  Let me
describe the simplest variant, which assumes that the word
vectors are related to word probabilities as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 3&lt;/em&gt; (&lt;strong&gt;Word2Vec(CBOW)&lt;/strong&gt;):&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\textstyle\Pr[w|w_1, w_2, \ldots, w_5] \propto \exp(v_w \cdot (\frac{1}{5} \sum_i v_{w_i})),\qquad (2)&lt;/script&gt;

  &lt;p&gt;where the left hand side gives the empirical probability that word $w$ occurs in the text
conditional on the last five words being $w_1$ through $w_5$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Assume we can estimate the left hand side using a large text corpus. Then expression (2) for the word vectors—together with  a constraint capping the dimension of the
vectors to, say, 300 — implicitly defines a nonconvex optimization problem which is solved in practice as follows. Let $S$ be the set of all the $6$-tuples of words that occur in the text. Let $N$ be a set of random $6$-tuples; this set is called the 
&lt;em&gt;negative sample&lt;/em&gt; since presumably these tuples are gibberish. The method consists of finding word embeddings that give high probability to 
tuples in $S$ and low probability to tuples in $N$. Roughly speaking, they maximise the &lt;em&gt;difference&lt;/em&gt; between the following two quantities: (i) sum of $\exp(v_w \cdot (\frac{1}{5} \sum_i v_{w_i}))$ (suitably scaled) over all $6$-tuples in $S$, and
(ii) the corresponding sum over tuples in $N$.  The &lt;strong&gt;word2vec&lt;/strong&gt; team introduced some other tweaks that allowed them to solve this optimization for very large corpora containing 10 billion words.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;word2vec&lt;/strong&gt; papers are a bit mysterious, and have motivated much
followup work. A paper by Levy and Goldberg (See &lt;a href=&quot;https://levyomer.wordpress.com/&quot;&gt;Omer Levy’s Blog&lt;/a&gt;) explains that the &lt;strong&gt;word2vec&lt;/strong&gt;
methods are actually modern versions of older vector space methods.
After all, if you take logs of both sides of expression (2), you see
that the &lt;em&gt;logarithm&lt;/em&gt; of some cooccurence probability is
being expressed in terms of inner products of some word vectors, which
is very much in the spirit of the older work. (Levy and Goldberg have more to say about this, backed up by interesting experiments comparing the vectors obtained by various approaches.)&lt;/p&gt;

&lt;p&gt;Another paper by Pennington et al. at Stanford suggests a &lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;model called GLOVE&lt;/a&gt;
that uses an explicit weighted-SVD strategy for finding word embeddings.
They also give an intuitive explanation of why these embeddings solve 
word analogy tasks, though the explanation isn’t quite rigorous.&lt;/p&gt;

&lt;p&gt;In a future post I will talk more about our &lt;a href=&quot;http://arxiv.org/abs/1502.03520&quot;&gt;subsequent theoretical work&lt;/a&gt; that tries to unify these different approaches, and also explains some  cool linear algebraic properties of word
embeddings. I note that linear structure  also arises in &lt;a href=&quot;http://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf&quot;&gt;representations of images learnt
via deep learning.&lt;/a&gt;, and it is tantalising to wonder if similar theory applies to that setting.&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Dec 2015 01:00:00 -0800</pubDate>
        <link>http://offconvex.github.io/2015/12/12/word-embeddings-1/</link>
        <guid isPermaLink="true">http://offconvex.github.io/2015/12/12/word-embeddings-1/</guid>
      </item>
     
    
     
      <item>
        <title>Why go off the convex path?</title>
        <description>&lt;p&gt;The notion of convexity underlies a lot of beautiful mathematics. When combined with computation, it gives rise to the area of convex optimization that has had a huge impact on understanding and improving the world we live in. However, convexity does not provide all the answers. Many procedures in statistics, machine learning and nature at large—Bayesian inference, deep learning, protein folding—successfully solve non-convex problems that are NP-hard, i.e., intractable on worst-case instances. Moreover, often nature or humans choose methods that are inefficient in the worst case to solve problems in &lt;a href=&quot;https://en.wikipedia.org/wiki/P_(complexity)&quot;&gt;P&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Can we develop a theory to resolve this mismatch between reality and the predictions of worst-case analysis?  Such a theory could identify structure in natural inputs that helps sidestep worst-case complexity.&lt;/p&gt;

&lt;p&gt;This blog is dedicated to the idea that optimization methods—whether created by humans or nature, whether convex or nonconvex—are exciting objects of study and, often lead to useful algorithms and insights into nature. This study can be seen as an extension of classical mathematical fields such as dynamical systems and differential equations among others, but with the important addition of the notion of computational efficiency.&lt;/p&gt;

&lt;p&gt;We will report on interesting research directions and open problems, and highlight progress that has been made. We will write articles ourselves as well as encourage others to contribute. In doing so, we hope to generate an active dialog between theorists, scientists and practitioners and to motivate a generation of young researchers to work on these important problems.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Dec 2015 03:34:00 -0800</pubDate>
        <link>http://offconvex.github.io/2015/12/11/mission-statement/</link>
        <guid isPermaLink="true">http://offconvex.github.io/2015/12/11/mission-statement/</guid>
      </item>
     
    
     
    
  </channel>
</rss>
